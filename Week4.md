So, there we saw that the classifier that we built for classifying fashion, like using convolutions were able to make it more efficient and to make it more accurate. I thought that was really, really cool, but it's still very limited in the scenario because all of our images are 28 by 28 and the subject is actually centered. And while it's a fabulous dataset for learning, it's like when we start getting into real-world images and complex images that maybe we need to go a little bit further. What do you think? I think it's really cool that taking the core idea of a confinet allows you to implement an algorithm to confine not just handbags right in the middle of the image but anywhere in the image, so it could be carried by someone on the left or the right of a much bigger and, say, a one-megapixel image. This is 1000 by 1000 pixels. Also for many applications rather than using grayscale, want to use color images- All right. And the same core ideas but with a bigger dataset, bigger images in similar labels lets you do that. All right. So, the technique that you're learning in this, is really really helping you to be able to succeed in these more real-world scenarios. So, I know you've been working on a dataset on horses- Yeah. And humans. Yeah, that's been a lot of fun. I've been working on a dataset that's a number of images of horses and they're moving around the image and they're in different poses, and humans in the same way and diverse humans male, female, different races, that kind of thing to see if we can build a binary classifier between the two of them. But what was really interesting about this is that they're all computer-generated images, but we can use them to classify real photos. I had a lot of fun with that. So, I think there'll be a fun exercise for you to work on as well. And if you're ever wondering of these algorithms you're learning whether this is the real stuff, the algorithims you're learning is really the real stuff that is used today in many commercial applications. For example, if you look at the way a real self-driving car today uses cameras to detect other vehicles or pedestrians to try to avoid them, they use convolutional neural networks for that part of the task, very similar to what you are learning. And in fact, in other contexts, I've heard you speak about using a convolutional neural network. To take a picture, for example. Yeah, we can take a picture of a crop and try to tell if it has a disease coming. So, that was really cool. Oh, thank you, thank you. That's really fun. So, in the next video, you'll learn how to apply convolutional neural networks to these much bigger and more complex images. Please go on to the next video.


As Andrew and Laurence discussed, the techniques you’ve learned already can apply to complex images, and you can start solving real scenarios with them. They discussed how it could be used, for example, in disease detection with the Cassava plant, and you can see a video demonstrating that here. Once you’ve watched that, move onto the next lesson!
https://www.youtube.com/watch?v=NlpS-DhayQA

To this point, you built an image classifier that worked using a deep neural network and you saw how to improve its performance by adding convolutions. One limitation though was that it used a dataset of very uniform images. Images of clothing that was staged and framed in 28 by 28. But what happens when you use larger images and where the feature might be in different locations? For example, how about these images of horses and humans? They have different sizes and different aspect ratios. The subject can be in different locations. In some cases, there may even be multiple subjects. In addition to that, the earlier examples with a fashion data used a built-in dataset. All of the data was handily split into training and test sets for you and labels were available. In many scenarios, that's not going to be the case and you'll have to do it for yourself. So in this lesson, we'll take a look at some of the APIs that are available to make that easier for you. In particular, the image generator in TensorFlow. One feature of the image generator is that you can point it at a directory and then the sub-directories of that will automatically generate labels for you. So for example, consider this directory structure. You have an images directory and in that, you have sub-directories for training and validation. When you put sub-directories in these for horses and humans and store the requisite images in there, the image generator can create a feeder for those images and auto label them for you. So for example, if I point an image generator at the training directory, the labels will be horses and humans and all of the images in each directory will be loaded and labeled accordingly. Similarly, if I point one at the validation directory, the same thing will happen. So let's take a look at this in code. The image generator class is available in Keras.preprocessing.image. You can then instantiate an image generator like this. I'm going to pass rescale to it to normalize the data. You can then call the flow from directory method on it to get it to load images from that directory and its sub-directories. It's a common mistake that people point the generator at the sub-directory. It will fail in that circumstance. You should always point it at the directory that contains sub-directories that contain your images. The names of the sub-directories will be the labels for your images that are contained within them. So make sure that the directory you're pointing to is the correct one. You put it in the second parameter like this. Now, images might come in all shapes and sizes and unfortunately for training a neural network, the input data all has to be the same size, so the images will need to be resized to make them consistent. The nice thing about this code is that the images are resized for you as they're loaded. So you don't need to preprocess thousands of images on your file system. But you could have done that if you wanted to. The advantage of doing it at runtime like this is that you can then experiment with different sizes without impacting your source data. While the horses and humans dataset is already in 300 by 300, when you use other datasets they may not always be uniformly sized. So this is really useful for you. The images will be loaded for training and validation in batches where it's more efficient than doing it one by one. Now, there's a whole science to calculating batch size that's beyond the scope of this course, but you can experiment with different sizes to see the impact on the performance by changing this parameter. Finally, there's the class mode. Now, this is a binary classifier i.e. it picks between two different things; horses and humans, so we specify that here. Other options in particular for more than two things will be explored later in the course. The validation generator should be exactly the same except of course it points at a different directory, the one containing the sub-directories containing the test images. When you go through the workbook shortly, you'll see how to download the images as a zip, and then sort them into training and test sub-directories, and then put horses and humans sub-directories in each. That's just pure Python. It's not TensorFlow or any other deep learning stuff. But it's all explained for you in the notebook.

Now that you’ve seen how an ImageGenerator can flow images from a directory and perform operations such as resizing them on the fly, the next thing to do is design the neural network to handle these more complex images. You’ll see that in the next video.
